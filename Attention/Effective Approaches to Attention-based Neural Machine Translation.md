# Effective Approaches to Attention-based Neural Machine Translation(2015)
## Introduction
* 두가지 타입의 attention-based model을 고안
    * global attention: source 문장의 모든 단어를 고려
        * Bahdanau(2015) 모델과 유사하지만 구조적으로 더 단순하게 설계 
    * local attention: 각 시각마다 서로 다른 단어 subset만 고려
        * Xu(2015)에서 고안된 hard & soft attention model의 장점만 모아서 만든 모델
            * global이나 soft attention보다 계산량은 적고, hard attention과는 다르게 미분이 가능
* 다양한 alignment 함수에 대해서도 실험해 성능을 비교(content-based / location-based function)

## Neural Machine Translation
* ~~NMT 기법에 대한 Recent Works 소개~~
## 참고
https://tmaxai.github.io/post/luong-attention/
