# basic-is-the-best
## 기본 개념 다루기 위한 논문 정리
### 다양한 러닝 기법들
#### Gradient Descent
* [Zeiler, M. D. ADADELTA: An adaptive learning rate method(2012)](https://arxiv.org/abs/1212.5701)

### Attention
#### Seq2Seq
* [Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation(2014)](https://arxiv.org/abs/1406.1078)
* [Sutskever, I., Vinyals, O., and Le, Q. Sequence to sequence learning with neural networks(2014)](https://arxiv.org/abs/1409.3215)

#### Align(Attention) 매커니즘
* [Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate(2015)](https://arxiv.org/abs/1409.0473#:~:text=Neural%20machine%20translation%20is%20a,to%20maximize%20the%20translation%20performance.)
