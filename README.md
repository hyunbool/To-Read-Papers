# To-Read-Papersüë©‚Äçüíª
ÏùΩÏñ¥Î≥ºÎßåÌïú & ÏùΩÏñ¥Î¥êÏïºÌï† & ÏùΩÏñ¥Î≥¥Í≥† Ïã∂ÏùÄ ÎÖºÎ¨∏ Ï†ïÎ¶¨

## Learning Methods
### Gradient Descent
* [Zeiler, M. D. ADADELTA: An adaptive learning rate method(2012)](https://arxiv.org/abs/1212.5701)

### ÏÑ±Îä• Í∞úÏÑ†
* [Jean et al. On Using Very Large Target Vocabulary for Neural Machine Translation(2014)](https://arxiv.org/pdf/1412.2007.pdf)
## Data Augmentation
* [Data Augmentation using Pre-trained Transformer Models(2020)](https://arxiv.org/pdf/2003.02245.pdf)
## Attention
### Seq2Seq
* [Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation(2014)](https://arxiv.org/abs/1406.1078)
* [Sutskever, I., Vinyals, O., and Le, Q. Sequence to sequence learning with neural networks(2014)](https://arxiv.org/abs/1409.3215)

### Align(Attention) Îß§Ïª§ÎãàÏ¶ò
* [Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate(2015)](https://arxiv.org/abs/1409.0473#:~:text=Neural%20machine%20translation%20is%20a,to%20maximize%20the%20translation%20performance.)
* [Minh-Thang Luong, Hieu Pham, Christopher D. Manning. Effective Approaches to Attention-based Neural Machine Translation(2015)](https://arxiv.org/abs/1508.04025)

#### Visual Attention
* [Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention(2015)](https://arxiv.org/abs/1502.03044)

#### Hierchical Attention
* [Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy. Hierarchical Attention Networks for Document Classification(2016)](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)

#### Self Attention
* [Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Attention Is All You Need(2017)](https://arxiv.org/abs/1706.03762)
    * Ï∞∏Í≥†ÌïòÎ©¥ Ï¢ãÏùÄ ÏÇ¨Ïù¥Ìä∏Îì§
        1. http://nlp.seas.harvard.edu/2018/04/03/attention.html
        2. https://wikidocs.net/31379
        3. http://jalammar.github.io/illustrated-transformer/
* [ARE TRANSFORMERS UNIVERSAL APPROXIMATORS OF SEQUENCE-TO-SEQUENCE FUNCTIONS?(2019)](https://arxiv.org/abs/1912.10077)

## Language Models
* [Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)](https://arxiv.org/abs/1810.04805)
    * Ï∞∏Í≥†ÏÇ¨Ïù¥Ìä∏
        1. http://docs.likejazz.com/bert/
        2. https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3
* [Improving Language Understanding by Generative Pre-Training(2018)](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
* [Language Models are Unsupervised Multitask Learners(2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [XLNet: Generalized Autoregressive Pretraining for Language Understanding(2019)](https://arxiv.org/pdf/1906.08237.pdf)

## Reinforcement Learning
|Title|Summary|
|---|---|
|[Playing Atari with Deep Reinforcement Learning(2013)](https://arxiv.org/pdf/1312.5602.pdf)| |
