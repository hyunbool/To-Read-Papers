# Papers that covers some basic conceptsğŸ‘©â€ğŸ’»
ë…¼ë¬¸ ì½ë‹¤ê°€ í—·ê°ˆë¦¬ëŠ” ê°œë… ì •ë¦¬í•˜ê¸°
## Learning Methods
### Gradient Descent
* [Zeiler, M. D. ADADELTA: An adaptive learning rate method(2012)](https://arxiv.org/abs/1212.5701)

## Attention
### Seq2Seq
* [Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation(2014)](https://arxiv.org/abs/1406.1078)
* [Sutskever, I., Vinyals, O., and Le, Q. Sequence to sequence learning with neural networks(2014)](https://arxiv.org/abs/1409.3215)

### Align(Attention) ë§¤ì»¤ë‹ˆì¦˜
* [Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate(2015)](https://arxiv.org/abs/1409.0473#:~:text=Neural%20machine%20translation%20is%20a,to%20maximize%20the%20translation%20performance.)
* [Minh-Thang Luong, Hieu Pham, Christopher D. Manning. Effective Approaches to Attention-based Neural Machine Translation(2015)](https://arxiv.org/abs/1508.04025)

#### Visual Attention
* [Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention(2015)](https://arxiv.org/abs/1502.03044)

#### Hierchical Attention
* [Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy. Hierarchical Attention Networks for Document Classification(2016)](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)

#### Self Attention
* [Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Attention Is All You Need(2017)](https://arxiv.org/abs/1706.03762)
    * ì°¸ê³ í•˜ë©´ ì¢‹ì€ ì‚¬ì´íŠ¸ë“¤
        1. http://nlp.seas.harvard.edu/2018/04/03/attention.html
        2. https://wikidocs.net/31379
        3. http://jalammar.github.io/illustrated-transformer/

## Language Models
* [Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)](https://arxiv.org/abs/1810.04805)
    * ì°¸ê³ ì‚¬ì´íŠ¸
        1. http://docs.likejazz.com/bert/
        2. https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3
