# 기본 개념 다루기 위한 논문 정리
논문 읽다가 헷갈리는 개념 정리하기
## 다양한 러닝 기법들
### Gradient Descent
* [Zeiler, M. D. ADADELTA: An adaptive learning rate method(2012)](https://arxiv.org/abs/1212.5701)

## Attention
### Seq2Seq
* [Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation(2014)](https://arxiv.org/abs/1406.1078)
* [Sutskever, I., Vinyals, O., and Le, Q. Sequence to sequence learning with neural networks(2014)](https://arxiv.org/abs/1409.3215)

### Align(Attention) 매커니즘
* [Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate(2015)](https://arxiv.org/abs/1409.0473#:~:text=Neural%20machine%20translation%20is%20a,to%20maximize%20the%20translation%20performance.)
* [Minh-Thang Luong, Hieu Pham, Christopher D. Manning. Effective Approaches to Attention-based Neural Machine Translation(2015)](https://arxiv.org/abs/1508.04025)

#### Visual Attention
* [Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention(2015)](https://arxiv.org/abs/1502.03044)

#### Hierchical Attention
* [Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy. Hierarchical Attention Networks for Document Classification(2016)](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)
